---
title: "Assessment 2"
author: "Joana Levtcheva, CID 01252821"
header-includes: \usepackage{amsmath}
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
```

```{r echo = FALSE}
library(tseries)
```

### Q1

- \begin{equation}\label{eq:eq1}\tag{1} 2X_t = \epsilon_t - X_{t-1} \end{equation}

We can rewrite (\ref{eq:eq1}) as:

\begin{equation}\label{eq:eq2}\tag{2}
X_t = - \frac{1}{2} X_{t-1} + \epsilon_t\end{equation}

This is a process of the type $X_t = \delta + \phi X_{t-1} + \epsilon_t$, where $\phi$ is a constant coefficient and $\phi \neq 0$, and $\epsilon_t$ are i.i.d. random variables with mean zero and variance $\sigma_{\epsilon}^{2}$. Therefore, (\ref{eq:eq2}) is a $AR(1)$ process, with $\phi = -\frac{1}{2}$. The $AR(1)$ process is stationary if and only if $|\phi| < 1$, which is true for $\phi = -\frac{1}{2}$. Therefore, (\ref{eq:eq1}) is stationary.

- \begin{equation}\label{eq:eq3}\tag{3} X_t = \epsilon_t + \epsilon_{t-1} + \epsilon_{t-2} \end{equation}

This is a process of the type $Y_t = \epsilon_t + \theta_1\epsilon_{t-1} + \theta_{2}\epsilon_{t-2}$, where $\theta_1, \theta_2$ are constants with $\theta_2 \neq 0$, and $\{\epsilon_t\}_t$ is a white noise process. Therefore (\ref{eq:eq3}) a $MA(2)$ process.

The process has mean $\mu = 0$. We are going to check if the process is stationary:

1. For the expectation we have \begin{align}E(X_t) = E( \epsilon_{t}) + E( \epsilon_{t-1}) + E( \epsilon_{t-2}) = 0 = \mu,\nonumber\end{align} which is finite and constant for all t. 

2. The varaince is \begin{align}Var[ X_t] = E[(X_t - \mu)^2] = E[ X_{t}^2] = E[( \epsilon_t + \epsilon_{t-1} + \epsilon_{t-2})^2] =\nonumber\end{align} \begin{align}= E[\epsilon_t^2 + \epsilon_{t-1}^2 + \epsilon_{t-2}^2 + 2\epsilon_{t}\epsilon_{t-1} + 2\epsilon_{t}\epsilon_{t-2} + 2\epsilon_{t-1}\epsilon_{t-2}] =\nonumber\end{align} \begin{align}= E[ \epsilon_t^2] + E[ \epsilon_{t-1}^2] + E[ \epsilon_{t-2}^2] + 2E[ \epsilon_{t}\epsilon_{t-1}] + 2E[ \epsilon_{t}\epsilon_{t-2}] + 2E[ \epsilon_{t-1}\epsilon_{t-2}] =\nonumber\end{align} \begin{align}= 3\sigma^2,\nonumber\end{align} where $\sigma$ is the variance of the white noise process. Therefore, $Var[ X_t]$ is finite for all t.

3. Finally, \begin{align}C_{XX}(t_i, t_i + h) = E[(X_{t_i} - \mu)(X_{t_i + h} - \mu)] =\nonumber\end{align} \begin{align}= E[(X_{t_i})(X_{t_i + h})] = E[( \epsilon_{t_i} + \epsilon_{t_i - 1} + \epsilon_{t_i -2})( \epsilon_{t_i + h} + \epsilon_{t_i + h -1} + \epsilon_{t_i + h-2})] = 0,\nonumber\end{align} which holds for every $h > 0$ and every $t_i \in T$. For $h = 0$ we have \begin{align}C_{XX}(t_i, t_i) = Var[X_{t_i}] = 3\sigma^2.\nonumber\end{align} Therefore $C_{XX}(t_1, t_1 + h) = C_{XX}(t_2, t_2 + h)$ for all $t_1, t_2 \in T$.

From 1., 2., and 3. it follows that the process (\ref{eq:eq3}) is stationary.

- \begin{equation}\label{eq:eq4}\tag{4} X_t = \sin( \frac{2\pi t}{10}) + \epsilon_t \end{equation}

First, we are going to show that (\ref{eq:eq4}) is an $AR(2)$ process.

Let's observe the process \begin{equation}\label{eq:eq5}\tag{5} X_{t+1} = \sin(2\pi \phi (t+1)) + \epsilon_{t+1} \end{equation}. We know that \begin{align}\sin(a+b) + \sin(a-b) = 2\cos(b)\sin(a).\nonumber\end{align} We have \begin{align}\sin(2\pi \phi (t+1)) = \sin(2\pi \phi t + 2\pi \phi) = 2\cos(2\pi \phi)\sin(2\pi \phi t) - \sin(2\pi \phi t - 2\pi \phi).\nonumber\end{align} For (\ref{eq:eq4}) we get \begin{align}X_{t+1} = 2\cos( \frac{2\pi}{10})\sin( \frac{2\pi t}{10}) + (-1)\sin( \frac{2\pi (t-1)}{10}) + \epsilon_{t+1}.\nonumber\end{align} Or \begin{align}X_{t+1} = 2\cos( \frac{2\pi}{10})X_t + (-1)X_{t-1} + \epsilon_{t}.\nonumber\end{align} This is the same as \begin{align}X_{t} = 2\cos( \frac{2\pi}{10})X_{t-1} + (-1)X_{t-2} + \epsilon_{t}.\nonumber\end{align}

This is an $AR(2)$ process with coefficients $\phi_1 = 2\cos( \frac{2\pi}{10}) \neq 0$ and $\phi_2 = -1$.

An $AR(2)$ process is stationary provided all the roots of the following polynomial equation have an absolute value greater than 1, they are outside of the unit circle: \begin{align}1 - \phi_1 z - \phi_2 z^2 = 0.\nonumber\end{align} We have \begin{align}1 - 2\cos( \frac{2\pi}{10})z + z^2 = 0\nonumber\end{align} with solutions $z_{1,2} = \frac{1+\sqrt{5}}{4} \pm i\frac{\sqrt{10-2\sqrt5}}{4}$ having an absolute value of 1. Therefore, (\ref{eq:eq4}) is not stationary.

### Q2

#### Part a

The Kwiatkowski-Phillips-Schmidt-Shin (KPSS) test is a test of stationarity. The KPSS test assumes that the time series of interest can be decomposed to a deterministic trend, random walk and stationary process, i.e. $\{X_t\}_{t>0}$ with $$X_t = \delta_t + r_t + u_t,$$ $$r_t = r_{t-1} + w_t ,$$ where $\delta_t$ denotes the deterministic trend, $r_t$ denotes the random walk, and $u_t$ denotes the stationary process, and $w_t$ is an i.i.d. process with mean 0, and variance $\sigma^2$.

The KPSS test can test for trend-stationarity against the null hypothesis, $$H_0: \text{The time series is trend-stationary, or }\sigma^2 = 0.$$ and with an alternative hypothesis $$H_1: \text{The time series contains a unit root, or } \sigma^2 > 0.$$

Note: The KPSS test can also test for level-stationarity.

#### Part b

Loading data, plotting data and ACF:

```{r Fig1, fig.height=10, fig.width=15}
par(mfrow = c(3, 1))
ts_data <- read.table("time-series.csv", header = T, sep = ",")
ts_data <- as.ts(unlist(ts_data[,-1]))
str(ts_data)
plot(ts_data, type='o')
acf(ts_data, lag.max = 30)
pacf(ts_data, lag.max = 30)
```

Based on the ACF the process seems to have both (slight) trend and seasonality.

Performing ADF test:

```{r}
adf.test(ts_data)
```

The p-value is 0.01 with the default lag order $k = 5$, and at the $5%$ significance level we have that it is smaller than 0.05. Therefore, there is evidence against non-stationarity.

The k parameter is a set of lags added to address serial correlation and it can vary depending on the order $p$. The value of $k$ should be chosen such taht $p \leq k-1$. The default lag order in `adf.test` is calculated by $trunc((length(x)-1)^{\frac{1}{3}})$. In our case the default lag order is $k = 5$, which leads $p$ to ideally be $p \leq 4$. Therefore, if the true value of $p$ is less than 5 the test would perform as expected and would reject the null hypothesis as well, whereas if it is significantly bigger the test would return the opposite result.

```{r}
adf.test(ts_data, k = 2)
adf.test(ts_data, k = 3)
adf.test(ts_data, k = 7)
adf.test(ts_data, k = 15)
adf.test(ts_data, k = 30)
```

The results are different depending on whether the lag order is bigger or smaller than the default lag order. Depending on the true value of $p$ they can be right or wrong. For example, if the true value of $p$ is less than or equal to the default value of $k$ the correct result would be of rejecting the null hypothsis, but if the true value of $p$ turns out to be somewhat bigger than the default value of $k$ the correct result would be the opposite.

#### Part c

Performing KPSS test for trend-stationarity:

```{r}
kpss.test(ts_data, null = "Trend")
```

The p-value is 0.07073 with the default lag order $k = 4$. Since we are evaluating at the 5% significance level and this value is not less than 0.05, we fail to reject the null hypothesis of the KPSS test. This means we can assume that the time series is trend stationary.

### Q3

Wickham, Hadley., "A Layered Grammar of Graphics" (2010), Journal of Computational and Graphical Statistics, vol. 19, no. 1, pp. 3â€“28

Keywords: grammar of graphics, ggplot2, statistical graphics, R

In "A Layered Grammar of Graphics" the author presents a graphical grammar which allows us to move beyond the famous named graphics (such as scatterplot) and enables us to gain insight into the composition of complicated statistical graphics and their underlying deep structure, making it possible to describe their components; the grammar also reveals "unexpected connections between seemingly different graphics".

The author is comparing and constrasting tha layered grammar to Wilkinson's grammar ("The Grammar of Graphics") and walks us through introducing the grammar (showing how to build a basic plot, and a more complicated plot), defining the components of the layered grammar (such as layers (data and mapping, statistical transformation, geometric object, position adjustment), scales coordinate system, faceting), describing a hierarchy of defaults, mentioning embedding the grammar in the programming language R, and showing some implications of the layered grammar (histograms, polar coordinates, transformations), and finally Wickham discusses "some perceptual issues and thinking about how we can build on the grammar to learn how to create graphical 'poems'".

In an attempt to create a grammar which can be used to build higher level tools for data analysis, Wickham proposes an alternative parametrization of Wilkinson's grammar "based around the idea of building up a graphic from multiple layers of data" by creating a strong foundation for understanding a diverse range of graphics and defining what a well-formed or correct graphics looks like.

The author adopts a formal and technical tone, relatively easy to be understood by his audience, the readers of "A Layered Grammar of Graphics" (2010) published in Journal of Computational and Graphical Statistics, vol. 19, others interested in the topic of building a graphical grammar, the users of the ggplot2 library, and people interested in further building on the grammar Wickham proposed. 
