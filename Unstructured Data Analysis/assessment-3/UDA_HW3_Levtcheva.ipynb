{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "np.random.seed(70103)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.7320508075688772"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.spatial.distance import euclidean\n",
    "\n",
    "\n",
    "sentence = np.array([2, 1, 1])  # vector corresponding to \"see eye to eye\", words ordered alphabetically\n",
    "query = np.array([1, 0, 0])     # vector corresponding to \"eye\" following the above order\n",
    "\n",
    "# compute Euclidean distance\n",
    "euclidean_distance = euclidean(query, sentence)\n",
    "euclidean_distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.26726124191242434"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# define documents\n",
    "D1 = \"Sandwich is a common lunch in the UK.\"\n",
    "D2 = \"In Denmark, workers have lunch at midday.\"\n",
    "\n",
    "# vectorize the documents with a custom token pattern\n",
    "vectorizer = CountVectorizer(token_pattern=r\"(?u)\\b\\w+\\b\")\n",
    "vectors = vectorizer.fit_transform([D1, D2])\n",
    "\n",
    "pd.DataFrame(vectors.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "\n",
    "# compute cosine similarity\n",
    "cosine_sim = cosine_similarity(vectors[0], vectors[1])[0][0]\n",
    "cosine_sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>a</th>\n",
       "      <th>at</th>\n",
       "      <th>common</th>\n",
       "      <th>denmark</th>\n",
       "      <th>have</th>\n",
       "      <th>in</th>\n",
       "      <th>is</th>\n",
       "      <th>lunch</th>\n",
       "      <th>midday</th>\n",
       "      <th>sandwich</th>\n",
       "      <th>the</th>\n",
       "      <th>uk</th>\n",
       "      <th>workers</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>D1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>D2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    a  at  common  denmark  have  in  is  lunch  midday  sandwich  the  uk  \\\n",
       "D1  1   0       1        0     0   1   1      1       0         1    1   1   \n",
       "D2  0   1       0        1     1   1   0      1       1         0    0   0   \n",
       "\n",
       "    workers  \n",
       "D1        0  \n",
       "D2        1  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(vectors.toarray(), columns=vectorizer.get_feature_names_out(), index=[\"D1\", \"D2\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>a</th>\n",
       "      <th>at</th>\n",
       "      <th>common</th>\n",
       "      <th>denmark</th>\n",
       "      <th>have</th>\n",
       "      <th>in</th>\n",
       "      <th>is</th>\n",
       "      <th>lunch</th>\n",
       "      <th>midday</th>\n",
       "      <th>sandwich</th>\n",
       "      <th>the</th>\n",
       "      <th>uk</th>\n",
       "      <th>workers</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>D1</th>\n",
       "      <td>0.378</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.378</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.269</td>\n",
       "      <td>0.378</td>\n",
       "      <td>0.269</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.378</td>\n",
       "      <td>0.378</td>\n",
       "      <td>0.378</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>D2</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.408</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.408</td>\n",
       "      <td>0.408</td>\n",
       "      <td>0.290</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.290</td>\n",
       "      <td>0.408</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.408</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       a    at  common  denmark  have    in    is  lunch  midday  sandwich  \\\n",
       "D1 0.378 0.000   0.378    0.000 0.000 0.269 0.378  0.269   0.000     0.378   \n",
       "D2 0.000 0.408   0.000    0.408 0.408 0.290 0.000  0.290   0.408     0.000   \n",
       "\n",
       "     the    uk  workers  \n",
       "D1 0.378 0.378    0.000  \n",
       "D2 0.000 0.000    0.408  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(token_pattern=r\"(?u)\\b\\w+\\b\")\n",
    "# compute the TF-IDF matrix\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform([D1, D2])\n",
    "\n",
    "# convert the matrix to a dense format and get feature names\n",
    "tfidf_dense = tfidf_matrix.todense()\n",
    "feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "tfidf_df = pd.DataFrame(tfidf_dense, columns=feature_names, index=[\"D1\", \"D2\"])\n",
    "pd.set_option('display.float_format', lambda x: '%.3f' % x)\n",
    "tfidf_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.pipeline import make_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>autumn is my favorite time of year to cook! th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>this recipe calls for the crust to be prebaked...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>this modified version of 'mom's' chili was a h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>this is a super easy, great tasting, make ahea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>my dh's amish mother raised him on this recipe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1006</th>\n",
       "      <td>classic from florida's famous columbia restaur...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1007</th>\n",
       "      <td>this is from cajun-recipes.com.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1008</th>\n",
       "      <td>perfect for a warm summer day, this is from a ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1009</th>\n",
       "      <td>i found this recipe in a cookbook of old ameri...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1010</th>\n",
       "      <td>the orange extract is optional, but it makes a...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            description\n",
       "0     autumn is my favorite time of year to cook! th...\n",
       "1     this recipe calls for the crust to be prebaked...\n",
       "2     this modified version of 'mom's' chili was a h...\n",
       "3     this is a super easy, great tasting, make ahea...\n",
       "4     my dh's amish mother raised him on this recipe...\n",
       "...                                                 ...\n",
       "1006  classic from florida's famous columbia restaur...\n",
       "1007                    this is from cajun-recipes.com.\n",
       "1008  perfect for a warm summer day, this is from a ...\n",
       "1009  i found this recipe in a cookbook of old ameri...\n",
       "1010  the orange extract is optional, but it makes a...\n",
       "\n",
       "[1000 rows x 1 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load data, assuming the file RAW_recipes.csv is in the same directory as the notebook\n",
    "file_path = 'RAW_recipes.csv'\n",
    "data = pd.read_csv(file_path)\n",
    "descriptions_raw = data[data[\"description\"].notna()][:1000][[\"description\"]]  # get the first not NA 1000 descriptions\n",
    "descriptions_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/joanalevtcheva/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/joanalevtcheva/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# download NLTK resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # convert to lowercase\n",
    "    text = text.lower()\n",
    "    # remove numbers and symbols\n",
    "    text = re.sub(r'[\\d\\W]+', ' ', text)\n",
    "    # remove stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = text.split()\n",
    "    words = [word for word in words if word not in stop_words]\n",
    "    # perform stemming\n",
    "    stemmer = PorterStemmer()\n",
    "    words = [stemmer.stem(word) for word in words]\n",
    "\n",
    "    return ' '.join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       autumn favorit time year cook recip prepar eit...\n",
       "1       recip call crust prebak bit ad ingredi feel fr...\n",
       "2       modifi version mom chili hit christma parti ma...\n",
       "3       super easi great tast make ahead side dish loo...\n",
       "4       dh amish mother rais recip much prefer store b...\n",
       "                              ...                        \n",
       "1006    classic florida famou columbia restaur whole r...\n",
       "1007                                      cajun recip com\n",
       "1008    perfect warm summer day forgotten recip cookbo...\n",
       "1009    found recip cookbook old american magazin reci...\n",
       "1010    orang extract option make ice complement flavo...\n",
       "Name: description, Length: 1000, dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "descriptions = descriptions_raw['description'].apply(preprocess_text)\n",
    "descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 100)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TF-IDF Vectorization\n",
    "tfidf_vectorizer = TfidfVectorizer(sublinear_tf=True, stop_words=\"english\")\n",
    "# SVD with top 100 singulare values\n",
    "svd = TruncatedSVD(100)\n",
    "lsa = make_pipeline(tfidf_vectorizer, svd)\n",
    "\n",
    "# fit LSA\n",
    "X = lsa.fit_transform(descriptions)\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0: recip, make, easi, use, like, time, good, love, tast, great\n",
      "Topic 1: easi, quick, dinner, super, make, bread, dish, kraft, great, good\n",
      "Topic 2: good, cake, love, chocol, recip, got, quick, everyon, simpl, tri\n",
      "Topic 3: good, soup, serv, use, chicken, low, fat, cream, meat, bean\n",
      "Topic 4: delici, tri, look, kraft, dish, magazin, tasti, time, cook, good\n",
      "Topic 5: cake, bread, tri, time, chocol, delici, includ, cooki, flour, wheat\n",
      "Topic 6: delici, cake, like, tri, love, dish, look, make, tast, nice\n",
      "Topic 7: bread, soup, tri, delici, recip, love, look, bean, wheat, wonder\n",
      "Topic 8: great, time, tast, kid, magazin, love, look, tri, day, good\n",
      "Topic 9: good, year, famili, time, make, delici, includ, favorit, simpl, bread\n",
      "Topic 10: dish, serv, quick, cake, bread, meal, favorit, fat, famili, delici\n",
      "Topic 11: tast, wonder, serv, like, dessert, salad, easi, food, famili, best\n",
      "Topic 12: tri, salad, dinner, cake, great, sauc, serv, food, dress, use\n",
      "Topic 13: quick, fat, tast, delici, love, low, minut, chicken, tri, kid\n",
      "Topic 14: soup, cake, kraft, love, cream, chicken, got, famili, serv, delici\n",
      "Topic 15: favorit, year, easi, chocol, ago, soup, tast, friend, best, magazin\n",
      "Topic 16: salad, nice, kraft, magazin, light, tasti, use, low, fat, wonder\n",
      "Topic 17: serv, magazin, food, tasti, tri, sauc, like, chees, kraft, kid\n",
      "Topic 18: sauc, magazin, day, make, chocol, best, tast, cooki, delici, got\n",
      "Topic 19: cake, chicken, sauc, tasti, bread, old, easi, enjoy, like, year\n",
      "Topic 20: food, day, love, free, comfort, cooki, delici, flavor, everyon, add\n",
      "Topic 21: cooki, chocol, got, dish, salad, quick, use, best, dinner, wonder\n",
      "Topic 22: famili, like, simpl, quick, magazin, look, cook, bread, kraft, pass\n",
      "Topic 23: love, chocol, salad, simpl, chicken, cooki, easi, chip, tasti, serv\n",
      "Topic 24: realli, yummi, cook, got, ingredi, famili, food, serv, bread, salad\n",
      "Topic 25: realli, day, serv, hot, simpl, tast, use, want, kid, meal\n",
      "Topic 26: tasti, serv, look, chicken, make, want, cooki, came, nice, cake\n",
      "Topic 27: day, favorit, mom, yummi, kid, alway, fat, use, easi, cake\n",
      "Topic 28: kid, chocol, sauc, look, yummi, time, wonder, simpl, salad, littl\n",
      "Topic 29: com, good, meal, day, salad, chicken, make, fast, fri, came\n",
      "Topic 30: kid, food, rice, wonder, chicken, came, dish, cook, alway, comfort\n",
      "Topic 31: realli, make, com, tasti, magazin, chocol, wonder, minut, rice, soup\n",
      "Topic 32: com, simpl, www, wonder, magazin, bake, believ, friend, fat, http\n",
      "Topic 33: want, tast, came, super, low, chicken, enjoy, favorit, husband, eat\n",
      "Topic 34: best, work, rice, friend, delici, banana, eat, want, know, serv\n",
      "Topic 35: flavor, dinner, cook, salad, like, came, quick, post, kraft, add\n",
      "Topic 36: cream, simpl, quick, meal, toast, sour, french, yummi, flavor, food\n",
      "Topic 37: super, hot, cook, chicken, cold, anoth, best, sure, kid, salad\n",
      "Topic 38: magazin, cream, rice, flavor, fast, version, cook, kid, cooki, easi\n",
      "Topic 39: ingredi, free, cookbook, sure, dessert, magazin, crust, favorit, anoth, light\n",
      "Topic 40: minut, rice, light, famili, cooki, cream, need, soup, hour, ice\n",
      "Topic 41: free, flavor, got, year, use, magazin, serv, dish, egg, pepper\n",
      "Topic 42: enjoy, tasti, kraft, meal, creami, anoth, famili, dip, banana, minut\n",
      "Topic 43: minut, cookbook, chocol, famili, magazin, friend, chicken, come, yummi, believ\n",
      "Topic 44: want, cook, hot, muffin, chocol, bean, vegetarian, spici, famili, husband\n",
      "Topic 45: cookbook, friend, cooki, favorit, kraft, kitchen, spici, burger, want, websit\n",
      "Topic 46: super, free, chicken, flavor, littl, cream, best, anoth, famili, websit\n",
      "Topic 47: muffin, sweet, dinner, healthi, nice, differ, year, look, super, eat\n",
      "Topic 48: eat, healthi, flavor, magazin, favorit, work, dessert, dinner, nice, famili\n",
      "Topic 49: think, magazin, cooki, anoth, yummi, kid, kraft, came, minut, soup\n",
      "Topic 50: yummi, year, bake, super, potato, ago, post, healthi, refriger, french\n",
      "Topic 51: nice, minut, favorit, kraft, kid, cook, parti, peopl, websit, need\n",
      "Topic 52: healthi, enjoy, store, fresh, littl, minut, got, delici, simpl, tasti\n",
      "Topic 53: best, vegetarian, spici, healthi, use, ask, tasti, pie, dinner, look\n",
      "Topic 54: friend, super, meal, nice, came, mix, think, pasta, come, salad\n",
      "Topic 55: flavor, mix, fri, better, come, spici, say, favorit, food, came\n",
      "Topic 56: better, cookbook, toast, think, similar, enjoy, best, bake, lot, come\n",
      "Topic 57: nice, came, magazin, come, best, sweet, alway, way, rice, toast\n",
      "Topic 58: come, sure, work, meal, eat, say, light, believ, cook, enjoy\n",
      "Topic 59: cookbook, flavor, say, version, tasti, dinner, fri, bake, like, need\n",
      "Topic 60: littl, need, tasti, favorit, think, husband, spici, roll, shrimp, flour\n",
      "Topic 61: free, dip, meat, enjoy, eat, spici, friend, vegetarian, think, pork\n",
      "Topic 62: toast, chocol, eat, alway, muffin, way, ask, french, post, tast\n",
      "Topic 63: super, meal, light, differ, fish, corn, muffin, moist, better, crunchi\n",
      "Topic 64: husband, cup, butter, flour, cooki, tasti, say, cookbook, pancak, prepar\n",
      "Topic 65: mom, potato, pie, fresh, came, husband, littl, differ, pasta, italian\n",
      "Topic 66: healthi, toast, flavor, vegetarian, littl, salad, french, super, vegan, come\n",
      "Topic 67: add, like, tasti, work, burger, muffin, magazin, spici, fridg, chop\n",
      "Topic 68: came, bean, vegetarian, corn, meal, burger, pork, cooki, week, pot\n",
      "Topic 69: everyon, anoth, rice, day, want, favorit, hope, mix, old, think\n",
      "Topic 70: best, muffin, com, cookbook, bake, light, look, littl, morn, flavor\n",
      "Topic 71: mix, wonder, kitchen, better, kraft, think, fri, eat, adapt, real\n",
      "Topic 72: dessert, come, differ, ingredi, pasta, italian, meal, bean, sound, request\n",
      "Topic 73: pie, ask, better, dessert, peopl, say, shrimp, meal, cookbook, add\n",
      "Topic 74: post, say, prepar, husband, ask, anoth, restaur, bar, magazin, need\n",
      "Topic 75: want, pot, share, oatmeal, parti, cup, pork, cream, fri, chop\n",
      "Topic 76: post, nice, vegan, work, bean, fresh, kitchen, got, juic, parti\n",
      "Topic 77: chees, calori, sandwich, egg, share, anoth, know, layer, taken, leftov\n",
      "Topic 78: butter, shrimp, toast, free, muffin, sure, frozen, pasta, french, look\n",
      "Topic 79: want, chicken, need, vegan, sandwich, vegetarian, burger, frozen, chang, meal\n",
      "Topic 80: bean, differ, sure, box, hour, surpris, pizza, best, cup, place\n",
      "Topic 81: bar, shrimp, fridg, dip, differ, leftov, dessert, roll, sweet, refriger\n",
      "Topic 82: leftov, pie, spici, someth, eat, toast, differ, pepper, come, creami\n",
      "Topic 83: pork, fresh, famili, dip, ingredi, egg, season, know, treat, cooki\n",
      "Topic 84: work, came, bake, vegetarian, burger, real, crab, protein, believ, think\n",
      "Topic 85: book, refriger, calori, sure, sweet, drink, eat, alway, dip, pizza\n",
      "Topic 86: littl, meal, ahead, add, hot, websit, look, mother, sandwich, low\n",
      "Topic 87: book, burger, includ, calori, wonder, bar, cheesecak, year, togeth, dinner\n",
      "Topic 88: similar, fridg, look, come, creami, thing, someth, fast, vegan, bar\n",
      "Topic 89: adapt, bit, big, hour, fri, box, zaar, meatloaf, night, version\n",
      "Topic 90: anoth, share, hit, roll, flour, bought, store, onion, tortilla, tomato\n",
      "Topic 91: origin, dessert, parti, fresh, sweet, look, think, slice, actual, restaur\n",
      "Topic 92: parti, burger, healthi, websit, way, think, freez, perfect, low, freezer\n",
      "Topic 93: egg, kitchen, eat, tender, food, bar, thing, daughter, tortilla, roll\n",
      "Topic 94: roll, freez, fridg, dip, look, ad, work, version, mushroom, bar\n",
      "Topic 95: oatmeal, week, sure, biscuit, start, version, old, best, grandmoth, steak\n",
      "Topic 96: bean, share, believ, anoth, thing, say, burger, kitchen, salad, wheat\n",
      "Topic 97: hous, kitchen, ad, meal, leav, layer, restaur, spici, hot, everyon\n",
      "Topic 98: ingredi, week, bit, spici, pasta, meal, refriger, grain, pie, work\n",
      "Topic 99: better, book, snack, drink, pasta, readi, pepper, fri, substitut, dinner\n",
      "Explained Variance Ratio: 0.3321260041239851\n"
     ]
    }
   ],
   "source": [
    "# top terms for each component\n",
    "terms = tfidf_vectorizer.get_feature_names_out()\n",
    "for i, comp in enumerate(svd.components_):\n",
    "    terms_comp = zip(terms, comp)\n",
    "    sorted_terms = sorted(terms_comp, key=lambda x: x[1], reverse=True)[:10]\n",
    "    print(\"Topic \" + str(i) + \": \" + \", \".join([t[0] for t in sorted_terms]))\n",
    "\n",
    "print(\"Explained Variance Ratio:\", np.sum(svd.explained_variance_ratio_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.manifold import TSNE\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# # Reducing the dimensionality of the data to 2 dimensions using t-SNE for visualization\n",
    "# tsne = TSNE(n_components=2, random_state=0, perplexity=15)\n",
    "# X_tsne = tsne.fit_transform(X)\n",
    "\n",
    "# plt.figure(figsize=(8, 8))\n",
    "# plt.scatter(X_tsne[:, 0], X_tsne[:, 1], marker='.')\n",
    "# plt.title('t-SNE visualization of LSA topics')\n",
    "# plt.xlabel('t-SNE feature 1')\n",
    "# plt.ylabel('t-SNE feature 2')\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "good healthy low fat soup.\n",
      "170\n",
      "0.8051080803273803\n",
      "a super, veggie-packed salad.  courtesy of rachael ray.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "\n",
    "test_descr = data[-56:-55][\"description\"].values\n",
    "# test_descr = data[-66:-65][\"description\"].values\n",
    "# test_descr = data[-1:][\"description\"].values\n",
    "print(test_descr[0])\n",
    "res = lsa.transform(test_descr)\n",
    "cos_sim = cosine_similarity(X, res)\n",
    "\n",
    "argmax_cs = np.argmax(cos_sim)\n",
    "print(argmax_cs)\n",
    "print(cos_sim[argmax_cs][0])\n",
    "print(descriptions_raw[\"description\"][argmax_cs])\n",
    "\n",
    "# 613\n",
    "# 0.606620611753556"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "import nltk\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/joanalevtcheva/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/joanalevtcheva/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# downloading NLTK resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # convert to lowercase\n",
    "    text = text.lower()\n",
    "    # remove numbers and symbols\n",
    "    text = re.sub(r'[\\d\\W]+', ' ', text)\n",
    "    # remove stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = text.split()\n",
    "    words = [word for word in words if word not in stop_words]\n",
    "    # perform stemming\n",
    "    stemmer = PorterStemmer()\n",
    "    words = [stemmer.stem(word) for word in words]\n",
    "\n",
    "    return ' '.join(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Loading and Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>location</th>\n",
       "      <th>Date</th>\n",
       "      <th>Rating</th>\n",
       "      <th>Review</th>\n",
       "      <th>Image_Links</th>\n",
       "      <th>processed_review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Helen</td>\n",
       "      <td>Wichita Falls, TX</td>\n",
       "      <td>Reviewed Sept. 13, 2023</td>\n",
       "      <td>5.000</td>\n",
       "      <td>Amber and LaDonna at the Starbucks on Southwes...</td>\n",
       "      <td>['No Images']</td>\n",
       "      <td>amber ladonna starbuck southwest parkway alway...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Courtney</td>\n",
       "      <td>Apopka, FL</td>\n",
       "      <td>Reviewed July 16, 2023</td>\n",
       "      <td>5.000</td>\n",
       "      <td>** at the Starbucks by the fire station on 436...</td>\n",
       "      <td>['No Images']</td>\n",
       "      <td>starbuck fire station altamont spring fl made ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Daynelle</td>\n",
       "      <td>Cranberry Twp, PA</td>\n",
       "      <td>Reviewed July 5, 2023</td>\n",
       "      <td>5.000</td>\n",
       "      <td>I just wanted to go out of my way to recognize...</td>\n",
       "      <td>['https://media.consumeraffairs.com/files/cach...</td>\n",
       "      <td>want go way recogn starbuck employe billi fran...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Taylor</td>\n",
       "      <td>Seattle, WA</td>\n",
       "      <td>Reviewed May 26, 2023</td>\n",
       "      <td>5.000</td>\n",
       "      <td>Me and my friend were at Starbucks and my card...</td>\n",
       "      <td>['No Images']</td>\n",
       "      <td>friend starbuck card work thank worker paid dr...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Tenessa</td>\n",
       "      <td>Gresham, OR</td>\n",
       "      <td>Reviewed Jan. 22, 2023</td>\n",
       "      <td>5.000</td>\n",
       "      <td>I’m on this kick of drinking 5 cups of warm wa...</td>\n",
       "      <td>['https://media.consumeraffairs.com/files/cach...</td>\n",
       "      <td>kick drink cup warm water work instacart right...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>700</th>\n",
       "      <td>Margaret</td>\n",
       "      <td>Cotati, CA</td>\n",
       "      <td>Reviewed Oct. 2, 2011</td>\n",
       "      <td>1.000</td>\n",
       "      <td>I ordered Via Starbucks coffee online. I recei...</td>\n",
       "      <td>['No Images']</td>\n",
       "      <td>order via starbuck coffe onlin receiv email st...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>701</th>\n",
       "      <td>Ric</td>\n",
       "      <td>Oakville, ON</td>\n",
       "      <td>Reviewed Aug. 31, 2011</td>\n",
       "      <td>3.000</td>\n",
       "      <td>My name is Ric **, I am journalist by professi...</td>\n",
       "      <td>['No Images']</td>\n",
       "      <td>name ric journalist profess send letter starbu...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>702</th>\n",
       "      <td>Jayne</td>\n",
       "      <td>Ny, NY</td>\n",
       "      <td>Reviewed Aug. 24, 2011</td>\n",
       "      <td>1.000</td>\n",
       "      <td>The bagel was ice cold, not cut and not toasted.</td>\n",
       "      <td>['No Images']</td>\n",
       "      <td>bagel ice cold cut toast</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>703</th>\n",
       "      <td>Norma</td>\n",
       "      <td>La Puente, CA</td>\n",
       "      <td>Reviewed Aug. 15, 2011</td>\n",
       "      <td>1.000</td>\n",
       "      <td>In the morning of Monday, August 15, 2011, at ...</td>\n",
       "      <td>['No Images']</td>\n",
       "      <td>morn monday august co worker stop starbuck buy...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>749</th>\n",
       "      <td>Kenneth</td>\n",
       "      <td>Beecher, IL</td>\n",
       "      <td>Reviewed Feb. 6, 2010</td>\n",
       "      <td>5.000</td>\n",
       "      <td>I found the coffee at Starbucks overrated and ...</td>\n",
       "      <td>['No Images']</td>\n",
       "      <td>found coffe starbuck overr tast survey bitter ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>703 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         name           location                     Date  Rating  \\\n",
       "0       Helen  Wichita Falls, TX  Reviewed Sept. 13, 2023   5.000   \n",
       "1    Courtney         Apopka, FL   Reviewed July 16, 2023   5.000   \n",
       "2    Daynelle  Cranberry Twp, PA    Reviewed July 5, 2023   5.000   \n",
       "3      Taylor        Seattle, WA    Reviewed May 26, 2023   5.000   \n",
       "4     Tenessa        Gresham, OR   Reviewed Jan. 22, 2023   5.000   \n",
       "..        ...                ...                      ...     ...   \n",
       "700  Margaret         Cotati, CA    Reviewed Oct. 2, 2011   1.000   \n",
       "701       Ric       Oakville, ON   Reviewed Aug. 31, 2011   3.000   \n",
       "702     Jayne             Ny, NY   Reviewed Aug. 24, 2011   1.000   \n",
       "703     Norma      La Puente, CA   Reviewed Aug. 15, 2011   1.000   \n",
       "749   Kenneth        Beecher, IL    Reviewed Feb. 6, 2010   5.000   \n",
       "\n",
       "                                                Review  \\\n",
       "0    Amber and LaDonna at the Starbucks on Southwes...   \n",
       "1    ** at the Starbucks by the fire station on 436...   \n",
       "2    I just wanted to go out of my way to recognize...   \n",
       "3    Me and my friend were at Starbucks and my card...   \n",
       "4    I’m on this kick of drinking 5 cups of warm wa...   \n",
       "..                                                 ...   \n",
       "700  I ordered Via Starbucks coffee online. I recei...   \n",
       "701  My name is Ric **, I am journalist by professi...   \n",
       "702   The bagel was ice cold, not cut and not toasted.   \n",
       "703  In the morning of Monday, August 15, 2011, at ...   \n",
       "749  I found the coffee at Starbucks overrated and ...   \n",
       "\n",
       "                                           Image_Links  \\\n",
       "0                                        ['No Images']   \n",
       "1                                        ['No Images']   \n",
       "2    ['https://media.consumeraffairs.com/files/cach...   \n",
       "3                                        ['No Images']   \n",
       "4    ['https://media.consumeraffairs.com/files/cach...   \n",
       "..                                                 ...   \n",
       "700                                      ['No Images']   \n",
       "701                                      ['No Images']   \n",
       "702                                      ['No Images']   \n",
       "703                                      ['No Images']   \n",
       "749                                      ['No Images']   \n",
       "\n",
       "                                      processed_review  sentiment  \n",
       "0    amber ladonna starbuck southwest parkway alway...          1  \n",
       "1    starbuck fire station altamont spring fl made ...          1  \n",
       "2    want go way recogn starbuck employe billi fran...          1  \n",
       "3    friend starbuck card work thank worker paid dr...          1  \n",
       "4    kick drink cup warm water work instacart right...          1  \n",
       "..                                                 ...        ...  \n",
       "700  order via starbuck coffe onlin receiv email st...          0  \n",
       "701  name ric journalist profess send letter starbu...          0  \n",
       "702                           bagel ice cold cut toast          0  \n",
       "703  morn monday august co worker stop starbuck buy...          0  \n",
       "749  found coffe starbuck overr tast survey bitter ...          1  \n",
       "\n",
       "[703 rows x 8 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the provided dataset\n",
    "file_path = 'reviews_data.csv'\n",
    "data = pd.read_csv(file_path)\n",
    "data = data[(data[\"Rating\"].notna()) & (data[\"Review\"] != \"No Review Text\")]\n",
    "data['processed_review'] = data['Review'].apply(preprocess_text)\n",
    "# consider ratings 4 and 5 as positive (1) and the rest as negative (0)\n",
    "data['sentiment'] = data['Rating'].apply(lambda x: 1 if x > 3 else 0)\n",
    "data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extracting features and labels\n",
    "reviews = data['processed_review']\n",
    "sentiments = data['sentiment']\n",
    "\n",
    "# apply TF-IDF\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=100)\n",
    "features = tfidf_vectorizer.fit_transform(reviews)\n",
    "\n",
    "# splitting the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, sentiments, test_size=0.3, random_state=42)\n",
    "\n",
    "# train Logistic Regression\n",
    "log_reg_model = LogisticRegression()\n",
    "log_reg_model.fit(X_train, y_train)\n",
    "\n",
    "# predicting and evaluating the model\n",
    "y_pred = log_reg_model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "report = classification_report(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8293838862559242"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['positive', 'negative']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_texts = [\"My experience at Starbucks is great\", \"The Americano tastes awful\"]\n",
    "# transform the test data using the same TF-IDF vectorizer used for training the logistic regression model\n",
    "test_features_lr = tfidf_vectorizer.transform(test_texts)\n",
    "\n",
    "# predict sentiment\n",
    "predictions_lr = log_reg_model.predict(test_features_lr)\n",
    "\n",
    "# map numerical predictions back to sentiment labels\n",
    "predicted_sentiments_lr = [\"positive\" if prediction == 1 else \"negative\" for prediction in predictions_lr]\n",
    "predicted_sentiments_lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "uda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
