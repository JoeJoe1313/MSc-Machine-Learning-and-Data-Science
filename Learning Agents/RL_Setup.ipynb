{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8587c233",
   "metadata": {},
   "source": [
    "# RL Setup\n",
    "In order to implement our RL algorithms, we first set up a way of initializing our environment. In particular, we set up a class FiniteMDP which sets up an MDP with $S, A$ and $H$ being the number of states, actions and stages in each episode repsectively. We will have reward matrix $R\\in \\mathbb{R}^{S\\times A}$ representing the reward recieved for any action $a$ in any state $s$. We will also need to provide the transition probabilities $P \\in \\mathbb{R}^{S\\times A \\times S}$ as well as the initial state distribution $p_0$ for the MDP. We then choose all the necessary values to define and initialize an example MDP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8c3a2afe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "import numpy as np\n",
    "from scipy.special import softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1b1c17ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FiniteMDP: #(R, P, initial_state_distribution, H):\n",
    "    \"\"\"\n",
    "    Base class for a finite MDP.\n",
    "    Parameters\n",
    "    ----------\n",
    "    R : numpy.ndarray\n",
    "    P : numpy.ndarray\n",
    "    initial_state_distribution : numpy.ndarray or int\n",
    "        array of size (S,) containing the initial state distribution\n",
    "        or an integer representing the initial/default state\n",
    "    Attributes\n",
    "    ----------\n",
    "    R : numpy.ndarray\n",
    "        array of shape (S, A) containing the mean rewards, where\n",
    "        S = number of states;  A = number of actions.\n",
    "    P : numpy.ndarray\n",
    "        array of shape (S, A, S) containing the transition probabilities,\n",
    "        where P[s, a, s'] = Prob(S_{t+1}=s'| S_t = s, A_t = a).\n",
    "    H : horizon\n",
    "    sigma2 : noise variance.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, R, P, p0, H):\n",
    "        self.p0 = p0\n",
    "        S, A = R.shape\n",
    "\n",
    "        self.S = S\n",
    "        self.A = A\n",
    "        self.R = R\n",
    "        self.P = P\n",
    "        self.H = H\n",
    "\n",
    "        self.reward_range = (self.R.min(), self.R.max())\n",
    "\n",
    "        self.state = None\n",
    "\n",
    "        self.States = np.arange(S)\n",
    "        self.Actions = np.arange(A)\n",
    "\n",
    "    def ResetState(self):\n",
    "        \"\"\"\n",
    "        Reset the environment to a default state.\n",
    "        \"\"\"\n",
    "        self.state = np.random.choice(self.States,p=self.p0)\n",
    "\n",
    "    def Sample(self, state, action):\n",
    "        \"\"\"\n",
    "        Sample a transition s' from P(s'|state, action).\n",
    "        \"\"\"\n",
    "        prob = self.P[state, action, :]\n",
    "        next_state = np.random.choice(self.States, p=prob)\n",
    "        reward = self.R[state, action] \n",
    "        return next_state, reward\n",
    "\n",
    "    def Play(self, action):\n",
    "        next_state, reward = self.Sample(self.state, action)\n",
    "        self.state = next_state\n",
    "        return next_state, reward"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10fceb7a",
   "metadata": {},
   "source": [
    "### Dynamic Programming Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f2b1dc1",
   "metadata": {},
   "source": [
    "For most algorithms it will be helpful to have a function which will run the dynamic programming algorithm for a given transition and reward function (NB: we want the function to be general so these don't need to be the true ones).\n",
    "\n",
    "We recall the Dynamic Programming Algorithm from the notes in which we input the transition functions $P_h$ and reward functions $r_h$, in our case these are $P$ and $R$, and run the following steps.\n",
    "\n",
    "1. Set $V^*_{H+1}(x)=0$ for all $ x \\in \\mathcal{S}$\n",
    "2. For $h=H,\\dots, 1$:\n",
    "3. $\\qquad$ Calculate $ Q_h^*(x,a) = r_h(x,a) + \\sum_{x' \\in \\mathcal{S}} P_h(x'|x,a) V_{h+1}^*(x') $\n",
    "3. $\\qquad$ Set $\\pi^*_h(x) = \\text{argmax}_{a \\in \\mathcal{A}} Q_h^*(x,a)$\n",
    "4. $\\qquad$ Define $V_h^*(x) = \\text{max}_{a \\in \\mathcal{A}} Q_h^*(x,a) = Q_h^*(x,\\pi^*_h(x))$ \n",
    "\n",
    "We implement this in a function below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c64a0b1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define Dynamic Programming Algorithm\n",
    "def DP(MDP,P,R):\n",
    "    # define generally for any transition function P and reward R\n",
    "    H = MDP.H\n",
    "    Q = np.ones((H, S, A))\n",
    "    V = np.ones((H+1, S))\n",
    "    V[H, :] = 0\n",
    "    for h in range(H):\n",
    "        for s in MDP.States:\n",
    "            for a in MDP.Actions:\n",
    "                Q[H-h-1,s,a] = R[s,a] + P[s,a,:].dot(V[H-h,:])\n",
    "            \n",
    "            V[H-h-1,s] = np.max(Q[H-h-1,s,:])\n",
    "    \n",
    "    return Q, V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6578e36b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define function to find expected value of a policy defined using myQ in true MDP\n",
    "def ExpectedValue(myMDP,myQ):\n",
    "    H = myMDP.H\n",
    "    Q = np.ones((myMDP.H, myMDP.S, myMDP.A))\n",
    "    V = np.ones((myMDP.H+1, myMDP.S))\n",
    "    V[H, :] = 0\n",
    "    for h in range(H):\n",
    "        for s in myMDP.States:\n",
    "            for a in myMDP.Actions:\n",
    "                Q[H-h-1,s,a] = myMDP.R[s,a] + sum(myMDP.P[s,a,:]*V[H-h,:])\n",
    "            \n",
    "            myaction = np.argmax(myQ[H-h-1,s,:])\n",
    "            V[H-h-1,s] = Q[H-h-1,s,myaction]\n",
    "    \n",
    "    return Q, V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "17ff4aa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define function to add extra attributes to the MDP needed for the algorithms\n",
    "def AddExtraAttributes(myMDP):\n",
    "    # extract S,A,H from MDP\n",
    "    H = myMDP.H\n",
    "    S = myMDP.S\n",
    "    A = myMDP.A\n",
    "\n",
    "    # define Q and V functions attached to the MDP\n",
    "    setattr(myMDP, 'V', np.ones((H+1, S)))\n",
    "    myMDP.V[H, :] = 0\n",
    "    setattr(myMDP, 'Q', np.ones((H, S, A)))\n",
    "\n",
    "    for h in range(myMDP.H):\n",
    "        myMDP.V[h, :] *= (myMDP.H-h)\n",
    "        myMDP.Q[h, :, :] *= (myMDP.H-h)\n",
    "\n",
    "    # to run the DP algorithm for true MDP to get Vstar and Qstar\n",
    "    setattr(myMDP, 'Vstar', np.ones((H+1, S)))\n",
    "    myMDP.Vstar[H, :] = 0\n",
    "    setattr(myMDP, 'Qstar', np.ones((H, S, A)))\n",
    "    myMDP.Qstar, myMDP.Vstar = DP(myMDP,myMDP.P,myMDP.R)\n",
    "\n",
    "    # define estimates and counters to MDP\n",
    "    setattr(myMDP, 'Nsa', np.zeros((S, A)))\n",
    "    setattr(myMDP, 'Rhat', np.zeros((S, A)))\n",
    "    setattr(myMDP, 'Phat', np.zeros((S, A, S)))\n",
    "    setattr(myMDP, 'Vhat', np.ones((H+1, S)))\n",
    "    myMDP.Vhat[H, :] = 0\n",
    "    setattr(myMDP, 'Qhat', np.ones((H, S, A)))\n",
    "    setattr(myMDP, 'Vopt', np.ones((H+1, S)))\n",
    "    myMDP.Vopt[H, :] = 0\n",
    "    setattr(myMDP, 'Qopt', np.ones((H, S, A)))\n",
    "\n",
    "#AddExtraAttributes(myMDP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cdc4feb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function to generate a random environment \n",
    "def random_env(S,A,H):\n",
    "    R = np.random.uniform(0, 1, (S, A))\n",
    "    P = np.random.uniform(0, 1, (S, A, S))\n",
    "    p0 = np.ones(S)/S\n",
    "    # normalize transition probs     \n",
    "    for ss in range(S):\n",
    "        for aa in range(A):\n",
    "            P[ss, aa, :] /= P[ss, aa, :].sum()\n",
    "    return FiniteMDP(R, P, p0, H)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a58be5ed",
   "metadata": {},
   "source": [
    "### Define the standard Q-learning algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "de791a09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define alg running in T episodes of MDP, learning rate alpha=fn(Nsa)\n",
    "def Q_Learning(myMDP1, T, Nsa_fn, tau):\n",
    "    myMDP = copy.deepcopy(myMDP1)\n",
    "    reg = np.zeros(T)\n",
    "    rew = np.zeros(T)\n",
    "    true_reg = np.zeros(T)\n",
    "    # ResetCounters(myMDP)\n",
    "\n",
    "    # initialize Qhat,Vhat\n",
    "    myMDP.Qhat = np.ones((myMDP.H, myMDP.S, myMDP.A)) * myMDP.H\n",
    "    # myMDP.Qhat = 1.0*myMDP.Qstar\n",
    "    # myMDP.Vhat = 1.0*myMDP.Vstar\n",
    "    myMDP.Vhat = np.ones(((myMDP.H + 1, myMDP.S))) * myMDP.H\n",
    "    myMDP.Vhat[myMDP.H, :] = 0\n",
    "\n",
    "\n",
    "    episodes_to_reach_threshold = []\n",
    "    for t in range(T):\n",
    "        myMDP.ResetState()  # sample initial state\n",
    "        optval = myMDP.Vstar[0, myMDP.state]\n",
    "\n",
    "        # calculate expected value from playing with Qhat over episode\n",
    "        expval = ExpectedValue(myMDP, myMDP.Qhat)[1][0, myMDP.state]\n",
    "\n",
    "        eprew = 0.0\n",
    "        for h in range(myMDP.H):\n",
    "            myState = myMDP.state\n",
    "            # take action maximizing Qhat function\n",
    "            myAction = np.argmax(myMDP.Qhat[h, myState, :])\n",
    "            # record new state/reward from action played\n",
    "            mynextstate, myreward = myMDP.Play(myAction)\n",
    "            eprew += myreward\n",
    "            # UpdateEstMDP(myMDP, myState, myAction, mynextstate, myreward, h) #update N(s,a) counter\n",
    "            # Upate Qhat,Vhat\n",
    "            myMDP.Nsa[myState, myAction] += 1\n",
    "            alpha = Nsa_fn(myMDP.Nsa[myState, myAction])\n",
    "            myMDP.Qhat[h, myState, myAction] = (1 - alpha) * myMDP.Qhat[\n",
    "                h, myState, myAction\n",
    "            ] + alpha * (myreward + myMDP.Vhat[h + 1, mynextstate])\n",
    "            myMDP.Vhat[h, myState] = max(myMDP.Qhat[h, myState, :])\n",
    "\n",
    "            # check if threshold is exceeded\n",
    "            if eprew >= tau:\n",
    "                episodes_to_reach_threshold.append(t + 1)\n",
    "                break\n",
    "\n",
    "        reg[t] = optval - eprew\n",
    "        true_reg[t] = optval - expval\n",
    "        rew[t] = eprew\n",
    "\n",
    "    return rew, reg, true_reg, episodes_to_reach_threshold, myMDP.Qhat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acf9ea10",
   "metadata": {},
   "source": [
    "### Define the Softmax Q-learning algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "04f89ee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define alg running in T episodes of MDP, learning rate alpha, beta for softmax defn\n",
    "def Softmax_Q_Learning(myMDP1, T, Nsa_fn, beta, tau):\n",
    "    myMDP = copy.deepcopy(myMDP1)\n",
    "    reg = np.zeros(T)\n",
    "    rew = np.zeros(T)\n",
    "    # ResetCounters(myMDP)\n",
    "\n",
    "    # initialize Qhat,Vhat\n",
    "    myMDP.Qhat = np.ones((myMDP.H, myMDP.S, myMDP.A)) * myMDP.H\n",
    "    myMDP.Vhat = np.ones(((myMDP.H + 1, myMDP.S))) * myMDP.H\n",
    "    myMDP.Vhat[myMDP.H, :] = 0\n",
    "\n",
    "    episodes_to_reach_threshold = []\n",
    "    for t in range(T):\n",
    "        myMDP.ResetState()  # sample initial state\n",
    "        optval = myMDP.Vstar[0, myMDP.state]\n",
    "\n",
    "        eprew = 0.0\n",
    "        for h in range(myMDP.H):\n",
    "            myState = myMDP.state\n",
    "            # take action with softmax of Qhat function\n",
    "            probs = softmax(beta * myMDP.Qhat[h, myState, :])\n",
    "            myAction = np.random.choice(np.arange(myMDP.A), p=probs)\n",
    "            # record new state/reward from action played\n",
    "            mynextstate, myreward = myMDP.Play(myAction)\n",
    "            eprew += myreward\n",
    "            # Upate Qhat,Vhat\n",
    "            myMDP.Nsa[myState, myAction] += 1\n",
    "            alpha = Nsa_fn(myMDP.Nsa[myState, myAction])\n",
    "            myMDP.Qhat[h, myState, myAction] = (1 - alpha) * myMDP.Qhat[\n",
    "                h, myState, myAction\n",
    "            ] + alpha * (myreward + myMDP.Vhat[h + 1, mynextstate])\n",
    "            myMDP.Vhat[h, myState] = max(myMDP.Qhat[h, myState, :])\n",
    "\n",
    "            # check if threshold is exceeded\n",
    "            if eprew >= tau:\n",
    "                episodes_to_reach_threshold.append(t + 1)\n",
    "                break\n",
    "\n",
    "        reg[t] = optval - eprew\n",
    "        rew[t] = eprew\n",
    "\n",
    "    return rew, reg, episodes_to_reach_threshold, myMDP.Qhat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a98758c4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
